{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Probabilistic Model for YeastGates\n",
    "\n",
    "### Problem Challenge\n",
    "We have 5 strains in the YeastGates dataset: WT, XOR_00, XOR_01, XOR_10, XOR_11. Each strain has corresponding \"observed\" measurements, namely the flow cytometry data for a population-level study, RNAseq data for transcriptomics, and LC-MS data for proteomics and metabolomics. These omics capture data at **different granularities** (some are more high-throughput than others).\n",
    "Moreover, there is **inherent variation** in the datasets at multiple levels.\n",
    "1. within-experiment variations (biological and technical replicates)\n",
    "2. within-TA3 variations (robotic versus manual)\n",
    "3. inter-TA3 variations (slightly different protocols: data in log phase or after stationary phase?)\n",
    "\n",
    "*Research Question:* How do we principally account for these variations, and aggregate these **multiple omics** spaces to put out a predictive model of the YeastGates circuit?\n",
    "\n",
    "### Proposed Solution\n",
    "We use a probabilistic approach that:\n",
    "1. Identifies key variables that show noise\n",
    "2. Hypothesizes a latent space where the YeastGates strains \"live\" in a mixture model\n",
    "3. Generates every observed output space from the latent space through a probabilistic map\n",
    "4. Since we can marginalize over the latent space, we can work with data of any granularity\n",
    "\n",
    "To define this model, we need to define the random variables and the distributions they follow. We define the latent space to have a simple K-component Gaussian mixture prior distribution. We want the latent space to be as interpretable and constrained as possible (we shift most degrees of freedom to the probabilistic mappings), so we let it have 2 isotropic dimensions. But in general, every data-point $x_n \\in \\mathbb{R}^d$ in this d-dimensional latent space follows the following distribution:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_n | \\pi, \\mu, \\sigma \\sim \\sum_{k=1}^K \\pi_k Normal(x_n | \\mu_k, \\sigma_k^2 \\mathbf{1})\n",
    "\\end{equation*}\n",
    "\n",
    "To define the probabilistic map(s) from the input latent space to the output observed space, we tried multiple options. Namely a Guassian process mapping, which treats the map as a random variable from a Gaussian Process prior following a certain mean function and a covariance function defined by a kernel (like RBF) applied to the input space. However on toy datasets, we observed it was much more difficult to optimize. (Largely because being a non-parametric map, it doesn't scale to large datasets.) Next, we tried a simpler linear map, akin to probabilistic PCA. This is a parametric map, whose weights depend only on the dimensions of input/output spaces. (Interestingly, it is a special case of a Gaussian process map when using a linear covariance kernel.)\n",
    "\n",
    "So if we have a point in the latent space $x_n \\in \\mathbb{R}^D$, the corresponding point in the $j$th output space $y_n^j \\in \\mathbb{R}^P$ (with weight matrix $W^j \\in \\mathbb{R}^{P \\times D}$) is given by the distribution:\n",
    "\n",
    "\\begin{equation*}\n",
    "y_n^j | W^j,x_n,\\sigma \\sim Normal(y_n | W^jx_n^T, \\sigma^2 \\mathbf{1})\n",
    "\\end{equation*}\n",
    "\n",
    "(Note that we've skipped the prior distributions on each of the parameters of this model, for brevity. But the weights, mixture distribution $\\pi$, component distributions $\\mu$, $\\sigma$ have appropriate conjugate priors on them.)\n",
    "\n",
    "Once we define these maps for all output spaces, we can ask some interesting arbitrary probabilistic queries to this model. For example, \n",
    "1. Given a FACS readout, what is the likely YeastGates strain?\n",
    "2. Given a FACS readout, what is the likely metabolomic state?\n",
    "3. Given a FACS readout in one protocol space, what is it likely gonna look like in the other one?\n",
    "4. ...\n",
    "\n",
    "### Building the Model\n",
    "\n",
    "We use [Edward](http://edwardlib.org/), a deep probabilistic programming language built as a wrapper around [TensorFlow](https://www.tensorflow.org/), for writing our model and performing inference. This is a walkthrough to performing this analysis, which still needs some optimizing over! (Also, optimizing probabilistic models is kind of hard...)\n",
    "\n",
    "First, to install some key Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --user tensorflow\n",
    "pip install --user edward\n",
    "pip install --user plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we use the FACS data from one of the TA3 performers (UW BioFab) to demonstrate the model. Alternatively, you can use the toy dataset function to play with a small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd #to import data as a dataframe\n",
    "import numpy as np #for numerical computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#datapath\n",
    "facs_bioworks = ['/work/05263/sloomba/jupyter/sd2e-community/shared-q0-hackathon/yeast-gates/flowDataFrames/UWBiofab/UWBiofab_20170811_flowDataFrame.csv', \\\n",
    "                 '/work/05263/sloomba/jupyter/sd2e-community/shared-q0-hackathon/yeast-gates/flowDataFrames/UWBiofab/UWBiofab_20170929_flowDataFrame.csv']\n",
    "\n",
    "dfRun1 = pd.read_csv(facs_bioworks[0], index_col=0)\n",
    "dfRun2 = pd.read_csv(facs_bioworks[1], index_col=0)\n",
    "\n",
    "strains = ['WT', 'XOR_00', 'XOR_01', 'XOR_10', 'XOR_11'] #YeastGates families\n",
    "channels = ['FL1-A','FSC-A','SSC-A'] #we are only using the GFP fluorescence channel, and the forward and side scatters\n",
    "facs = []\n",
    "labels = []\n",
    "for i in range(len(strains)):\n",
    "    facs.append(dfRun1[dfRun1['Strain'] == strains[i]][channels])\n",
    "    labels.append(np.ones(np.shape(facs[-1])[0])*i)\n",
    "    facs.append(dfRun2[dfRun2['Strain'] == strains[i]][channels])\n",
    "    labels.append(np.ones(np.shape(facs[-1])[0])*i)\n",
    "facs = [np.concatenate(facs)] #we use only one output space as a proof of principle, but in general this will be a list of multiple output spaces\n",
    "labels = [np.concatenate(labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the fixed hyperparameters of the model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FAMILIES = len(strains)\n",
    "NUM_LATENT_DIMS = 2\n",
    "SIZE_OBS_SPACES = [np.shape(fac)[1] for fac in facs] # <FACS1, FACS2, ...>\n",
    "NUM_OBS_SPACES = len(SIZE_OBS_SPACES)\n",
    "N_OBS_SPACES = [np.shape(fac)[0] for fac in facs] # number of data points in each observed space\n",
    "MINI_BATCH_SIZE = 100\n",
    "NUM_MCMC_SAMPLES = 10000\n",
    "y_train = facs\n",
    "z_train = labels\n",
    "\n",
    "MU_SCALE = 8.0 #variance of the prior on component means; the higher the more separate the clusters might be\n",
    "WT_SCALE = 4.0 #variance of the prior on map weights; the higher, the more the degree of freedom in weight space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#toy dataset\n",
    "def build_toy_dataset(fixed=True):\n",
    "    if fixed:\n",
    "        pi = [np.array([0.4, 0.6]), np.array([0.3, 0.7])]\n",
    "        mus = [[[1, 1], [-1, -1]], [[1.5, 1.25], [-0.75, -1.25]]]\n",
    "        stds = [[[0.1, 0.2], [0.2, 0.1]], [[0.2, 0.1], [0.1, 0.2]]]\n",
    "    else:\n",
    "        pi = [np.random.beta(1, 1, NUM_FAMILIES) for i in range(NUM_OBS_SPACES)]\n",
    "        pi = [pi_c/np.sum(pi_c) for pi_c in pi]\n",
    "        mus = [np.random.randn(NUM_FAMILIES, SIZE_OBS_SPACES[i]) for i in range(NUM_OBS_SPACES)]\n",
    "        stds = [np.random.gamma(1, 1, (NUM_FAMILIES, SIZE_OBS_SPACES[i])) for i in range(NUM_OBS_SPACES)]\n",
    "    y_trains = [np.zeros((N_OBS_SPACES[i], SIZE_OBS_SPACES[i]), dtype=np.float32) for i in range(NUM_OBS_SPACES)]\n",
    "    z_trains = [np.zeros(N_OBS_SPACES[i], dtype=np.int32) for i in range(NUM_OBS_SPACES)]\n",
    "    for i in range(NUM_OBS_SPACES):\n",
    "        for j in range(N_OBS_SPACES[i]):\n",
    "            k = np.argmax(np.random.multinomial(1, pi[i]))\n",
    "            y_trains[i][j, :] = np.random.multivariate_normal(mus[i][k], np.diag(stds[i][k]))\n",
    "            z_trains[i][j] = k\n",
    "    return (y_trains, z_trains)\n",
    "\n",
    "#y_train, z_train = build_toy_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the observed FACS data in a 3D space. Note that the data is quite large (~75K data-points), and so we plot only a random subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = np.random.choice(np.shape(y_train[0])[0], 500)\n",
    "traces = [go.Scatter3d(x = y_train[i][index,0], y = y_train[i][index,1], z = y_train[i][index,2], \\\n",
    "                     showlegend=False, mode = 'markers', marker=dict(size=10, color=z_train[i][index])) \\\n",
    "          for i in range(NUM_OBS_SPACES)]\n",
    "init_notebook_mode()\n",
    "iplot(traces, filename='training_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin building our model. We import TensorFlow and Edward libraries, and the model distributions which define our random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant('hello world') #to check if TensorFlow was imported correctly\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "\n",
    "import edward as ed\n",
    "from edward.models import Dirichlet, InverseGamma, MultivariateNormalDiag, Normal, ParamMixture, MultivariateNormalTriL, Gamma, \\\n",
    "PointMass, Empirical\n",
    "from edward.util import rbf # for the Gaussian Process kernel, not used at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the random variables of the latent space, following certain prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mixture Model on the Latent Space\n",
    "alpha = 1.0\n",
    "pi = Dirichlet(tf.constant([alpha]*NUM_FAMILIES))\n",
    "mu = Normal(loc=tf.zeros(NUM_LATENT_DIMS), scale=MU_SCALE*tf.ones(NUM_LATENT_DIMS), sample_shape=NUM_FAMILIES)\n",
    "sigma = InverseGamma(concentration=tf.ones(NUM_LATENT_DIMS), rate=tf.ones(NUM_LATENT_DIMS), sample_shape=NUM_FAMILIES)\n",
    "x_latent = [ParamMixture(mixing_weights=pi, component_params={'loc': mu, 'scale_diag': tf.sqrt(sigma)}, \\\n",
    "                        component_dist=MultivariateNormalDiag, sample_shape=MINI_BATCH_SIZE)]*NUM_OBS_SPACES\n",
    "z_assignments = [x.cat for x in x_latent] #this variable represents the assignment of a data-point to a certain YeastGates strain\n",
    "\n",
    "#x_latent = [Normal(loc=tf.zeros(NUM_LATENT_DIMS), scale=tf.ones(NUM_LATENT_DIMS), sample_shape=MINI_BATCH_SIZE)]*NUM_OBS_SPACES\n",
    "print(x_latent)\n",
    "print(z_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the random variables of the probabilistic map to output space(s), following certain prior distributions.\n",
    "\n",
    "(Note above that we have defined the sample shape for the latent space of a `MINI_BATCH_SIZE` size, and not the actual number of outputs. This is because we would be performing batch learning, as further explained below.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Probabilistic PCA map to latent space\n",
    "weights = [Normal(loc=tf.zeros([SIZE_OBS_SPACES[i], NUM_LATENT_DIMS]), scale=WT_SCALE*tf.ones([SIZE_OBS_SPACES[i], NUM_LATENT_DIMS])) \\\n",
    "           for i in range(NUM_OBS_SPACES)]\n",
    "y_obs = [Normal(loc=tf.matmul(weights[i], x_latent[i], transpose_b=True), scale=tf.Variable(tf.ones([SIZE_OBS_SPACES[i], MINI_BATCH_SIZE]))) \\\n",
    "         for i in range(NUM_OBS_SPACES)]\n",
    "                  \n",
    "print(weights)\n",
    "print(y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model definition is now in place. The \"learning\" in probabilistic models is basically the inference of the posterior distributions, given the observed data. There are different inference algorithm strategies that can be used, all of which involve defining a \"proxy\" distribution which captures the same aggregate statistics as the actual distribution. We'd be doing Gibbs sampling for the parameters of the mixture model, which is a kind of MCMC sampling strategy. We define empirical distributions for the unknown random variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inference of assignments in latent space\n",
    "pi_q = Empirical(params=tf.Variable(tf.ones([NUM_MCMC_SAMPLES, NUM_FAMILIES])/NUM_FAMILIES))\n",
    "mu_q = Empirical(params=tf.Variable(tf.zeros([NUM_MCMC_SAMPLES, NUM_FAMILIES, NUM_LATENT_DIMS])))\n",
    "sigma_q = Empirical(params=tf.Variable(tf.ones([NUM_MCMC_SAMPLES, NUM_FAMILIES, NUM_LATENT_DIMS])))\n",
    "\n",
    "print(pi)\n",
    "print(pi_q)\n",
    "print(mu)\n",
    "print(mu_q)\n",
    "print(sigma)\n",
    "print(sigma_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the actual and the proxy distributions should have the same shape!) Now we define similar proxy distributions for the parameters of the probabilistic map, namely the latent space points and the weights. We will be using a variational inference strategy here, which tries to maximize a \"simpler\" likelihood function than the actual one. We use the Normal distributions to define these simpler likelihoods. \n",
    "\n",
    "**Mini Batching:** Since some of our output spaces have a lot of data, like the FACS, inference can be very slow if we try to maximize the likelihood over all data-points at once. So, we do a mini-batching of our space. Which means we consider only a random subsample of the observations in one iteration of the inference algorithm, and choose a new one on the next one. Since our data in consideration keeps changing, we need to define \"placeholders\" for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#placeholders for mini-batching the code\n",
    "y_ph = [tf.placeholder(tf.float32, [SIZE_OBS_SPACES[i], MINI_BATCH_SIZE]) for i in range(NUM_OBS_SPACES)]\n",
    "z_ph = [tf.placeholder(tf.int32, MINI_BATCH_SIZE) for i in range(NUM_OBS_SPACES)]\n",
    "idx_ph = [tf.placeholder(tf.int32, MINI_BATCH_SIZE) for i in range(NUM_OBS_SPACES)]\n",
    "\n",
    "#approximate distributions for inference\n",
    "weights_variables = [[tf.Variable(tf.random_normal([SIZE_OBS_SPACES[i], NUM_LATENT_DIMS])), \\\n",
    "                     tf.Variable(tf.random_normal([SIZE_OBS_SPACES[i], NUM_LATENT_DIMS]))] for i in range(NUM_OBS_SPACES)]\n",
    "weights_q = [Normal(loc=weights_variables[i][0], scale=tf.nn.softplus(weights_variables[i][1])) for i in range(NUM_OBS_SPACES)]\n",
    "x_latent_variables = [[tf.Variable(tf.random_normal([N_OBS_SPACES[i], NUM_LATENT_DIMS])), \\\n",
    "                       tf.Variable(tf.random_normal([N_OBS_SPACES[i], NUM_LATENT_DIMS]))] for i in range(NUM_OBS_SPACES)]\n",
    "x_latent_q = [Normal(loc=tf.gather(x_latent_variables[i][0], idx_ph[i]), \\\n",
    "                     scale=tf.nn.softplus(tf.gather(x_latent_variables[i][1], idx_ph[i]))) for i in range(NUM_OBS_SPACES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined the proxy distributions, we are ready to tie them to the actual model distributions. We do this in Edward using dictionaries. We define the data dictionary, which ties the distributions of observed data with the placeholders of data. We define the variable dictionary, which ties the actual and proxy distributions of rest of the model parameters.\n",
    "\n",
    "Note that we would be using an EM-style strategy to perform inference, wherein we first infer the probabilistic map while keeping the mixture model constant, and then vice-versa, reiteratively. And so we define two sets of data and variable dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data and variable dicts for inferring mixture model\n",
    "data_dict1 = dict(zip(y_obs, y_ph))\n",
    "data_dict1.update(zip(z_assignments, z_ph))\n",
    "data_dict1.update({pi: pi_q, mu: mu_q, sigma: sigma_q})\n",
    "variable_dict1 = dict(zip(x_latent, x_latent_q))\n",
    "variable_dict1.update(zip(weights, weights_q))\n",
    "\n",
    "#data and variable dicts for inferring probabilistic map\n",
    "data_dict2 = dict(zip(y_obs, y_ph))\n",
    "data_dict2.update(zip(z_assignments, z_ph))\n",
    "data_dict2.update(zip(x_latent, x_latent_q))\n",
    "data_dict2.update(zip(weights, weights_q))\n",
    "variable_dict2 = {pi: pi_q, mu: mu_q, sigma: sigma_q}\n",
    "\n",
    "#defining the 2 inference engines\n",
    "inference_p = ed.Gibbs(variable_dict2, data=data_dict2)\n",
    "inference_x = ed.KLqp(variable_dict1, data=data_dict1)\n",
    "\n",
    "#defining a scaling dictionary, necessary for scaling updates in batch algorithms\n",
    "scaling_dict = dict(zip(x_latent, [float(sum(N_OBS_SPACES))/(MINI_BATCH_SIZE*NUM_OBS_SPACES)]*NUM_OBS_SPACES))\n",
    "scaling_dict.update(zip(y_obs, [float(sum(N_OBS_SPACES))/(MINI_BATCH_SIZE*NUM_OBS_SPACES)]*NUM_OBS_SPACES))\n",
    "\n",
    "#initializing the inference engines\n",
    "inference_p.initialize(scale=scaling_dict)\n",
    "inference_x.initialize(scale=scaling_dict, n_samples = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to begin inference. We define an Edward session to run the inference in, and iteratively do variational inference and Gibbs sampling to simultaneously learn the mixure model and probabilistic map(s).\n",
    "\n",
    "(Note, if the codeblock below throws a placeholder error, just rerun it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = ed.get_session() #init Edward sessions\n",
    "tf.global_variables_initializer().run() #to initialize all variables in the computational graph of TensorFlow\n",
    "for _ in range(inference_p.n_iter): #main inference loop\n",
    "    idx_batch = [np.random.choice(N_OBS_SPACES[i], MINI_BATCH_SIZE) for i in range(NUM_OBS_SPACES)] #find a random mini-batch\n",
    "    y_obs_batch = [np.transpose(y_train[i][idx_batch[i],:]) for i in range(NUM_OBS_SPACES)] #mini batch of observed data\n",
    "    z_ass_batch = [z_train[i][idx_batch[i]] for i in range(NUM_OBS_SPACES)] #mini batch of observed assignments\n",
    "    feeding_dict = dict(zip(y_ph, y_obs_batch)) #feeding the batched data to appropriate placeholder variables in a feeding dictionary\n",
    "    feeding_dict.update(zip(z_ph, z_ass_batch))\n",
    "    feeding_dict.update(zip(idx_ph, idx_batch))\n",
    "    for _ in range(5): #variational inference update \n",
    "        info_dict = inference_x.update(feed_dict=feeding_dict)\n",
    "    info_dict = inference_p.update(feed_dict=feeding_dict) #Gibbs sampling update\n",
    "    t = info_dict['t']\n",
    "    if t % 100 == 0:\n",
    "        print(\"\\nweights:\")\n",
    "        [print(weight[0].eval()) for weight in weights_variables]\n",
    "        print(\"pi\", tf.reduce_mean(pi_q.params, 0).eval())\n",
    "        print(\"mu\", tf.reduce_mean(mu_q.params, 0).eval())\n",
    "        print(\"sigma\", tf.reduce_mean(sigma_q.params, 0).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vary the number of iterations for our algorithm by changing the `NUM_MCMC_SAMPLES` variable defined above. We can now plot a small subset of the inferred latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.concatenate([x_latent_variables[i][0].eval() for i in range(NUM_OBS_SPACES)]) #evaluating the latent space\n",
    "w_test = np.concatenate([weights_variables[i][0].eval() for i in range(NUM_OBS_SPACES)]) #evaluating the weights of probabilistic map\n",
    "index = np.random.choice(np.shape(x_test)[0], MINI_BATCH_SIZE)\n",
    "x_test = x_test[index]\n",
    "traces = [go.Scatter(x = x_test[:,0], y = x_test[:,1], showlegend=False, mode = 'markers', \\\n",
    "                     marker=dict(size=10, color=z_train[0][index]))]\n",
    "init_notebook_mode()\n",
    "iplot(traces, filename='latent_space')\n",
    "print(np.shape(x_test))\n",
    "print(w_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also interested in finding out how well the YeastGates strains differentiate out in the latent space. This can be done by evaluating the mixture components. These components can also be ways of diagnosing how much the different observed spaces influence this latent space, alone or in combinations, thus providing a way to compare different heterogenous observed spaces in this latent model space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"pi\", tf.reduce_mean(pi_q.params, 0).eval())\n",
    "print(\"mu\", tf.reduce_mean(mu_q.params, 0).eval())\n",
    "print(\"sigma\", tf.reduce_mean(sigma_q.params, 0).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the latent space by plotting the component variances in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_components(means, covariances):\n",
    "    import math\n",
    "    color_iter = ['navy', 'cyan', 'cornflowerblue', 'gold', 'orange']\n",
    "    data = []\n",
    "    for (mean, covar, color) in (zip(means, covariances, color_iter)):\n",
    "        a = covar[0]\n",
    "        b = covar[1]\n",
    "        x_origin = mean[0]\n",
    "        y_origin = mean[1]\n",
    "        x_ = [ ]\n",
    "        y_ = [ ]    \n",
    "        for t in range(0,361,10):\n",
    "            x = a*(math.cos(math.radians(t))) + x_origin\n",
    "            x_.append(x)\n",
    "            y = b*(math.sin(math.radians(t))) + y_origin\n",
    "            y_.append(y)\n",
    "        data.append(go.Scatter(x=x_ , y=y_, mode='lines', showlegend=False, line=dict(color=color, width=2)))\n",
    "    return data\n",
    "\n",
    "init_notebook_mode()\n",
    "iplot(plot_components(tf.reduce_mean(mu_q.params, 0).eval(), tf.reduce_mean(sigma_q.params, 0).eval()), filename='latent_space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are still far from coming up with solid hypotheses on actual YeastGates data! But we tried to provide a principled probabilistic framework to aggregate arbitrary observed data collected from different sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
